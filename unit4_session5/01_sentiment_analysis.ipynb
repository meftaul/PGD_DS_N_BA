{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is sentiment analysis?\n",
    "\n",
    " - The process of computationally identifying and categorizing opinions expressed in a piece of text or document.\n",
    " - Determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NLTK](https://www.nltk.org/)\n",
    "\n",
    " - Natural Language Toolkit (NLTK) is a Python library for NLP.\n",
    "\n",
    "[SpaCy](https://spacy.io/)\n",
    "\n",
    " - SpaCy is an open-source software library for advanced Natural Language Processing, written in the programming languages Python and Cython.\n",
    "\n",
    "[Genism](https://radimrehurek.com/gensim/)\n",
    "\n",
    " - Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Steps of sentiment analysis\n",
    "\n",
    "01. Data Collection\n",
    "02. Data Preprocessing\n",
    "    - Lowercasing\n",
    "    - Tokenization\n",
    "    - Stopwords removal\n",
    "    - Remove punctuation\n",
    "03. Text Vectorization\n",
    "    - Bag of Words\n",
    "    - TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "    - WordEmbedding\n",
    "       - Word2Vec\n",
    "       - GloVe\n",
    "       - BERT\n",
    "04. Model Selection\n",
    "    - Naive Bayes\n",
    "    - Logistic Regression\n",
    "    - Support Vector Machine\n",
    "    - Random Forest\n",
    "    - Neural Network\n",
    "05. Model Evaluation\n",
    "06. Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "Training Data:\n",
    "- Positive Tweet 1: \"I love the new phone.\"\n",
    "- Positive Tweet 2: \"Great weather today!\"\n",
    "- Negative Tweet 1: \"I hate waiting.\"\n",
    "\n",
    "### Step 1: Tokenization\n",
    "Tokenization involves breaking down each tweet into individual words:\n",
    "\n",
    "- Positive Tweet 1: [\"I\", \"love\", \"the\", \"new\", \"phone\"]\n",
    "- Positive Tweet 2: [\"Great\", \"weather\", \"today\"]\n",
    "- Negative Tweet 1: [\"I\", \"hate\", \"waiting\"]\n",
    "\n",
    "### Step 2: Calculate Prior Probabilities\n",
    "Calculate the prior probabilities based on the training data:\n",
    "\n",
    "- $P(Positive) = \\frac{2}{3}$\n",
    "- $P(Negative) = \\frac{1}{3}$\n",
    "\n",
    "### Step 3: Calculate Likelihood\n",
    "Calculate the likelihood of each word given the sentiment:\n",
    "\n",
    "- For Positive sentiment:\n",
    "  - $P(I|Positive) = \\frac{1}{5}$\n",
    "  - $P(love|Positive) = \\frac{1}{5}$\n",
    "  - $P(the|Positive) = \\frac{1}{5}$\n",
    "  - $P(new|Positive) = \\frac{1}{5}$\n",
    "  - $P(phone|Positive) = \\frac{1}{5}$\n",
    "  - $P(Great|Positive) = \\frac{1}{5}$\n",
    "  - $P(weather|Positive) = \\frac{1}{5}$\n",
    "  - $P(today|Positive) = \\frac{1}{5}$\n",
    "- For Negative sentiment:\n",
    "  - $P(I|Negative) = \\frac{1}{3}$\n",
    "  - $P(hate|Negative) = \\frac{1}{3}$\n",
    "  - $P(waiting|Negative) = \\frac{1}{3}$\n",
    "\n",
    "### Step 4: Calculate Posterior Probabilities\n",
    "Now, suppose we have a new tweet: \"I love the great weather today.\"\n",
    "\n",
    "Calculate the posterior probabilities for both Positive and Negative sentiments:\n",
    "\n",
    "- For Positive sentiment:\n",
    "  - $P(D|Positive) = P(I|Positive) \\times P(love|Positive) \\times P(the|Positive) \\times P(great|Positive) \\times P(weather|Positive) \\times P(today|Positive) \\approx \\frac{1}{5} \\times \\frac{1}{5} \\times \\frac{1}{5} \\times \\frac{1}{5} \\times \\frac{1}{5} \\times \\frac{1}{5} = \\frac{1}{3125}$\n",
    "  - $P(D) = P(D|Positive) \\times P(Positive) + P(D|Negative) \\times P(Negative) \\approx \\frac{1}{3125} \\times \\frac{2}{3} + 0 \\times \\frac{1}{3} = \\frac{2}{4687}$\n",
    "  - $P(Positive|D) = \\frac{P(D|Positive) \\times P(Positive)}{P(D)} \\approx \\frac{\\frac{1}{3125} \\times \\frac{2}{3}}{\\frac{2}{4687}} = \\frac{1}{3125} \\times \\frac{2}{3} \\times \\frac{4687}{2} = \\frac{1}{3125} \\times 2343 \\approx 0.74976$\n",
    "- For Negative sentiment:\n",
    "  - $P(D|Negative) = P(I|Negative) \\times P(love|Negative) \\times P(the|Negative) \\times P(great|Negative) \\times P(weather|Negative) \\times P(\"today\"|Negative) \\approx \\frac{1}{3} \\times 0 \\times \\frac{1}{3} \\times 0 \\times \\frac{1}{3} \\times 0 = 0$\n",
    "  - $P(Negative|D) = \\frac{P(D|Negative) \\times P(Negative)}{P(D)} = 0$\n",
    "\n",
    "In this case, the model correctly predicts that the tweet is positive because \"love,\" \"great,\" \"weather,\" and \"today\" are words associated with positive sentiment in the training data.\n",
    "\n",
    "----\n",
    "\n",
    "01. Tokenization\n",
    "   - Split the text into individual words (tokens) e.g. $w_1, w_2, w_3, ..., w_n$\n",
    "02. Calculate prior probabilities based on training data\n",
    "   - $P(Positive) = \\frac{Number\\ of\\ Positive\\ Tweets}{Total\\ Number\\ of\\ Tweets}$\n",
    "   - $P(Negative) = \\frac{Number\\ of\\ Negative\\ Tweets}{Total\\ Number\\ of\\ Tweets}$\n",
    "03. Calculate likelihood based on training data\n",
    "   - For each sentiment, calculate the probability of each word appearing in a tweet of that sentiment\n",
    "      - $P(D|Positive) = P(w_1|Positive) * P(w_2|Positive) * P(w_3|Positive) * ... * P(w_n|Positive)$\n",
    "      - $P(D|Negative) = P(w_1|Negative) * P(w_2|Negative) * P(w_3|Negative) * ... * P(w_n|Negative)$\n",
    "04. Calculate posterior probabilities\n",
    "   - $P(Positive|D) = \\frac{P(D|Positive) * P(Positive)}{P(D)}$\n",
    "   - $P(Negative|D) = \\frac{P(D|Negative) * P(Negative)}{P(D)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: This is a great experience! | Predicted Sentiment: negative\n",
      "Text: I don't like it. | Predicted Sentiment: negative\n",
      "Text: It's okay. | Predicted Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Training data\n",
    "texts = [\"I love this product!\", \"This is terrible.\", \"Neutral review.\"]\n",
    "labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "# Create a Naive Bayes classifier pipeline\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "# Train the model\n",
    "model.fit(texts, labels)\n",
    "\n",
    "# Test data\n",
    "test_texts = [\"This is a great experience!\", \"I don't like it.\", \"It's okay.\"]\n",
    "\n",
    "# Predict sentiments\n",
    "predictions = model.predict(test_texts)\n",
    "\n",
    "# Display the results\n",
    "for text, sentiment in zip(test_texts, predictions):\n",
    "    print(f\"Text: {text} | Predicted Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example using NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/meftaul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/meftaul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/meftaul/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.308, 'pos': 0.692, 'compound': 0.6696}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentIntensityAnalyzer().polarity_scores(\"I love this product!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.577, 'neu': 0.423, 'pos': 0.0, 'compound': -0.6249}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentIntensityAnalyzer().polarity_scores(\"My worst day ever.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stop words and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sia.polarity_scores(text)['compound']\n",
    "\n",
    "    if sentiment_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: I love the product. It works really well!\n",
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "example_text = \"I love the product. It works really well!\"\n",
    "\n",
    "# Preprocess the text\n",
    "preprocessed_text = preprocess_text(example_text)\n",
    "\n",
    "# Perform sentiment analysis\n",
    "sentiment = analyze_sentiment(preprocessed_text)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original Text: {example_text}\")\n",
    "print(f\"Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SentimentIntensityAnalyzer` in NLTK is part of the Vader (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tool. Vader is specifically designed for analyzing sentiments in text data and is well-suited for social media texts, reviews, and other short text snippets. Here's a brief overview of how `SentimentIntensityAnalyzer` works:\n",
    "\n",
    "1. **Lexicon-based approach:**\n",
    "   Vader uses a pre-built lexicon (a dictionary) that contains words and their associated sentiment scores. The lexicon is crafted to handle sentiments expressed in various contexts, including emoticons, capitalization, and intensifiers.\n",
    "\n",
    "2. **Polarity Scores:**\n",
    "   The `SentimentIntensityAnalyzer` assigns a polarity score to each word in the text. The scores include:\n",
    "   - **Positive score:** The likelihood that the text expresses positive sentiment.\n",
    "   - **Neutral score:** The likelihood that the text is neutral.\n",
    "   - **Negative score:** The likelihood that the text expresses negative sentiment.\n",
    "   - **Compound score:** A combination of the three scores above, normalized to fall between -1 (most negative) and +1 (most positive).\n",
    "\n",
    "3. **Sentiment Classification:**\n",
    "   Based on the compound score, the sentiment of the text is classified into three categories:\n",
    "   - Positive\n",
    "   - Negative\n",
    "   - Neutral\n",
    "\n",
    "4. **Handling Intensifiers and Negations:**\n",
    "   Vader is designed to handle intensifiers (e.g., \"very good\") and negations (e.g., \"not bad\") effectively. It considers the impact of such words on the sentiment scores.\n",
    "\n",
    "The `polarity_scores` method returns a dictionary containing positive, neutral, negative, and compound scores. You can then interpret these scores to determine the overall sentiment of the text.\n",
    "\n",
    "Keep in mind that while Vader is efficient and easy to use, it may not be suitable for all types of text or domains. For more complex tasks or domain-specific sentiment analysis, you might consider using machine learning-based approaches with custom-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa (Robustly Optimized BERT Pretraining Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.091, 'neu': 0.736, 'pos': 0.172, 'compound': 0.4118}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example_text = \"I hated the movie. It was a disaster. Poor direction, bad acting.\"\n",
    "\n",
    "example_text = \"I felt energized within five minutes, but it lasted for only 30 minutes.I paid $50 for this product, I could have just drunk a cup of coffee and saved my money.\"\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_score = sia.polarity_scores(example_text)\n",
    "sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative score: 0.05258140340447426\n",
      "Neutral score: 0.23635177314281464\n",
      "Positive score: 0.7110668420791626\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer.encode(example_text, return_tensors='pt')\n",
    "output = model(encoded_text)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "# scores\n",
    "# # print positive negative and neutral scores\n",
    "print(f\"Negative score: {scores[0]}\")\n",
    "print(f\"Neutral score: {scores[1]}\")\n",
    "print(f\"Positive score: {scores[2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
